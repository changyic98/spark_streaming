{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN headline streaming\n",
    "In this demo, we will scrape CNN last 50 stories html by using BeautifulSoup. The headlines containing keyword \"Fast Facts\" are extracted and stored locally at log dir. Spark tesxtfilestreaming check the log dir every 2 sec to do word count on the headline.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yichenchang/Documents/workspace/git_submit/spark_streaming/log\n"
     ]
    }
   ],
   "source": [
    "import requests  \n",
    "from bs4 import BeautifulSoup \n",
    "import os \n",
    "out_path=os.path.dirname(os.path.realpath('__file__'))+\"/log/\"\n",
    "print out_path[:-1]\n",
    "def webscraping(fileindex,records,url='https://www.cnn.com/specials/last-50-stories',key='Fast Fact'):\n",
    "    #r=requests.get('https://www.cnn.com/specials/last-50-stories')\n",
    "    r=requests.get(url) \n",
    "    soup = BeautifulSoup(r.text, 'html.parser')  \n",
    "    #print soup\n",
    "    results = soup.find_all('span',attrs={'class':'cd__headline-text'})\n",
    "\n",
    "    file_name=\"headline%i.txt\"%fileindex\n",
    "    \n",
    "    f = open(out_path+file_name,'a')\n",
    "    for result in results:  \n",
    "        headline = result.text\n",
    "        if key in headline and headline not in records:\n",
    "            #print headline       \n",
    "            f.write('\\n' + headline)\n",
    "            records.append(headline)\n",
    "    f.close()\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark text file streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2018-01-31 07:42:26\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-01-31 07:42:28\n",
      "-------------------------------------------\n",
      "(u'', 2)\n",
      "(u'Sarah', 1)\n",
      "(u'Google', 1)\n",
      "(u'Guild', 1)\n",
      "(u'Awards', 2)\n",
      "(u'Academy', 1)\n",
      "(u'Screen', 1)\n",
      "(u'Fast', 6)\n",
      "(u'Rico', 1)\n",
      "(u'Investigation', 1)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-01-31 07:42:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-01-31 07:42:32\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-01-31 07:42:34\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-01-31 07:42:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2018-01-31 07:42:38\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "\n",
    "sc.stop()\n",
    "# Create a local StreamingContext and batch interval of 2 second\n",
    "sc = SparkContext(\"local\",\"fileNetworkWordCount\")\n",
    "ssc = StreamingContext(sc, 2)\n",
    "\n",
    "#read new_file from log dir\n",
    "lines = ssc.textFileStream(\"log/\")\n",
    "counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "          .map(lambda x: (x, 1))\\\n",
    "          .reduceByKey(lambda a, b: a+b)\n",
    "counts.pprint()\n",
    "\n",
    "ssc.start()             # Start the computation\n",
    "#ssc.awaitTermination()  # Wait for the computation to terminate\n",
    "\n",
    "import time\n",
    "\n",
    "i=0\n",
    "records=[]\n",
    "while True:\n",
    "    if i>10:\n",
    "        break\n",
    "    else:\n",
    "        #print \"scraping %i\"%i\n",
    "        i+=1\n",
    "        records=webscraping(fileindex=i,records=records)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future work\n",
    "1. Build structured data from web.\n",
    "\n",
    "2. Application of Apache flume\n",
    "\n",
    "# Reference\n",
    "web scraping from http://www.dataschool.io/python-web-scraping-of-president-trumps-lies/.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
